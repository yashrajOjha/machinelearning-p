{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5be570c",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In data processing, we will frequently uses libraries like pandas(to process datasets) and numpy(to perform scientific calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdc9a1",
   "metadata": {},
   "source": [
    "After importing libraries we need to load the dataset into a dataframe(easier to handle) using pandas.\n",
    "Any dataset for ML will have \n",
    "- Features (generally the first few columns)\n",
    "    - features is basically variables which has info which is used to predict\n",
    "- Labels (the last column)\n",
    "    - labels which is a dependent variable\n",
    "    \n",
    "After loading the dataset, we need to house the data into matrices, the features in one matrix and labels in another.\n",
    "Location of indexes is done using iloc() function, it will get indexes of columns and put it into the variables.\n",
    "X will have all the rows and columns up until the last column, meaning it has the features.\n",
    "Y will have rows and column value of the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ba0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables for accessing datasets file\n",
    "#this dataset varible is a data frame\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "#first 3 columns in our data set is features stored in x, last column is labels stored in y\n",
    "x = dataset.iloc[:,:-1].values \n",
    "#this creates a matrix of features and labels\n",
    "#first it is rows, after comma it is columns\n",
    "#iloc locates indexes, it will get indexes of columns we want to extract and put into x \n",
    "#':' is for range, without anything after or before it, everything is selected\n",
    "#:-1 selecting all the columns excluding last one (labels)\n",
    "y = dataset.iloc[:,-1].values\n",
    "#y has all rows of last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f72075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227f3a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87881c3",
   "metadata": {},
   "source": [
    "We use SciKit learn on a large scale in Machine Learning, so in the next code we use sklearn's impute class.\n",
    "\n",
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. \n",
    "\n",
    "Impute offers us the chances to modify such data by replacing them with mean or other mathematical functions like average, median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c1f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data, replacing missing salary by average of all the salaries\n",
    "#using scikit learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#we can replace the missing value by anything\n",
    "#missing_value is to specify what value we will replace, which is nan\n",
    "#second argument is what it will be replaced with\n",
    "#fit will connect and transform will replace\n",
    "imputer.fit(x[:,1:3])\n",
    "#only passing columns that are numeric and so :,1:, first column is string\n",
    "x[:,1:3]=imputer.transform(x[:,1:3])\n",
    "#transforms returns the two columns with replacements, which will replace the original matrix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e11ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b1f7b",
   "metadata": {},
   "source": [
    "#### Why encoding categorical data?\n",
    "\n",
    "ML model might be confused when it looks at the data, it might not\n",
    "be able to relate what kind of data it is.\n",
    "\n",
    "Machine learning tutorials will recommend or require that you prepare your data in specific ways before fitting a machine learning model.\n",
    "\n",
    "Categorical data are variables that contain label values rather than numeric values. \n",
    "   - For example: Country is a categorical data which has values like, Germany, France and England.\n",
    "   \n",
    "Many machine learning algorithms cannot operate on label data directly. This means that categorical data must be converted to a numerical form.\n",
    "\n",
    "There are two ways to convert:\n",
    "1. **Integer Encoding**: Labelling the data as integers like 1 for red, 2 for green, 3 for blue. But sometimes things cant be related using integers hence we use the next method.\n",
    "2. **One Hot Coding**: here we label the data in binary value, like 110 for red,.100 for blue, 101 for green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57cf0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
    "#we specify what kind of transformation we want to do, then what kind of encoding and third the indexes of the column\n",
    "#passthrough means keeping columns that wont be transformed\n",
    "#we will use fit transform which will fit the connection and transform the column \n",
    "x = np.array(ct.fit_transform(x))\n",
    "#fit transform doesnt return the data as numpy array and so we convert it using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8e1ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ce340",
   "metadata": {},
   "source": [
    "In the above output you can notice what the OneHotEncoder does, we turned the countries into a numerical value.\n",
    "\n",
    "---\n",
    "Now we will encode the dependent variable, meaning we will convert the yes and nos to 1 and 0, and so it is natural for us to use Integer Encoding **(Label Encoder)**, discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519f1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#not passing any arguments because we want the whole column converted\n",
    "#encoding it into binary value 1 and 0 for yes and no\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97ae4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c5b25",
   "metadata": {},
   "source": [
    "#### Why should we apply feature scaling before splitting the data set?\n",
    "Feature Scaling scales the data set on the same level so one feature doesnt dominate other feature when the model is learning.\n",
    "\n",
    "Information shouldn't be altered on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e05d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train [[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "x_test [[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "y_train [0 1 0 0 1 1 0 1]\n",
      "y_test [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)\n",
    "# Train test splits the features and labels data, that is x and y\n",
    "# We will then provide the split size to divide the data set into 80-20\n",
    "# random state makes sures that splitting is done in a same way through out again\n",
    "print(\"x_train\",x_train)\n",
    "print(\"x_test\",x_test)\n",
    "print(\"y_train\",y_train)\n",
    "print(\"y_test\",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53242cfe",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "- **Standardisation**:  which consists of subtracting each value of your feature by the mean of all the values of the feature and then dividing by the standard deviation.\n",
    "    - Will do the job all the time\n",
    "- **Normalization**: It consists of subtracting each value of your feature by the minimum value of the feature and then dividing by the difference between the maximum value of the feature and the minimum value of the feature.\n",
    "    - Specific situations when certain features follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d81770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "# We dont need to standardise dummy variables like country because it will fail to interpret its true meaning\n",
    "x_train[:,3:] = sc.fit_transform(x_train[:,3:])\n",
    "x_test[:,3:] = sc.transform(x_test[:,3:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2672ae",
   "metadata": {},
   "source": [
    "**fit_transform()**\n",
    "\n",
    "The **fit** method is calculating the mean and variance of each of the features present in our data. \n",
    "\n",
    "The **transform** method is transforming all the features using the respective mean and variance.\n",
    "\n",
    "---\n",
    "\n",
    "Using the **transform()** method we can use the same mean and variance from our training data to transform our test data. \n",
    "\n",
    "Thus, the parameters learned by our model using the training data will help us to transform our test data.\n",
    "\n",
    "---\n",
    "\n",
    "**Why not fit transform on test data?**\n",
    "\n",
    "Because we dont want our model to learn about our test data.\n",
    "\n",
    "If we will use the fit method on our test data too, we will compute a new mean and variance that is a new scale for each feature and our model will learn about our test data too. \n",
    "\n",
    "Thus, what we want to keep as a surprise is no longer unknown to our model and we will not get a good estimate of how our model is performing on the test (unseen) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18e3da24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8ff8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
