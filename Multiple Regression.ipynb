{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b01412",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression \n",
    "\n",
    "It is the same thing as a simple linear regression but with multiple independent variables.\n",
    "For example, a salary can be predicted using years of experience, number of hours worked, projects worked in.\n",
    "\n",
    "#### Dummy Variables\n",
    "\n",
    "In a data set, there are categorical variables, we create an artificial variable to represent an attribute with two or\n",
    "more distinct categories/levels.\n",
    "\n",
    "For example in a dataset we might have New York and California as two categorical variables, **rows that have New York will have dummy representation of 1, and rows that have California might have 0 as their inclusion won't affect a dependent variable**.\n",
    "\n",
    "Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. This means that we don’t need to write out separate equation models for each subgroup. \n",
    "\n",
    "| Category      | Dummy | Dummy |\n",
    "| ----------- | -----|----- |\n",
    "| California      | 0 | 1 |\n",
    "| New York  | 1 | 0 |\n",
    "| New York      | 1 | 0 |\n",
    "| California  | 0 | 1 |\n",
    "| California      | 0 | 1 |\n",
    "| New York  | 1 | 0 |\n",
    "\n",
    "Number of columns depends on the number of categorical values.\n",
    "\n",
    "    y = constant + co.eff1 x variable 1 + co.eff2 x variable 2 + co.eff3 x variable 3 + co.eff4 x dummy variable\n",
    "\n",
    "The default situation will be included in the constant, if we omit california because it is zero then it will be compensated in the constant.\n",
    "\n",
    "#### What is statistical significance?\n",
    "\n",
    "There are two hypothesis', \n",
    "- **null hypothesis**, where we assume things are fair, for example a coin is fair when it has both heads and tails. \n",
    "- **alternative hypothesis**, is when we see that things are not fair, a coin will be unfair when multiple tosses will result in a similar result.\n",
    "\n",
    "Let's toss a coin and see what happens, assuming that it is a fair coin,\n",
    "\n",
    "A coin is tossed once: we get tails then Probability is = .5\n",
    "\n",
    "A coin is tossed twice: we get tails again then P = .25\n",
    "\n",
    "A coin is tossed thrice: we get tails thrice then P = .12\n",
    "\n",
    "A coin is tossed 4 times: P = .06\n",
    "\n",
    "A coin is tossed 5 time: P = .03\n",
    "\n",
    "If the results keep repeating then we might start feeling uncomfortable because the probability of getting the same result always is very low realistically.\n",
    "\n",
    "The P value is dropping in a fair universe, but in an unfair universe it would have been 100% but it will always deliver same result and so we wouldn't feel uncomfortable, but in a fair universe chances of same result again and again is fairly unrealistic.\n",
    "\n",
    "The uneasy feeling must stop at a certain point, this is where alpha comes in, **if the P value drops below Alpha, we understand that the null hypothesis has failed**.\n",
    "\n",
    "- A small p (≤ 0.05), reject the null hypothesis. This is strong evidence that the null hypothesis is invalid.\n",
    "- A large p (> 0.05) means the alternate hypothesis is weak, so you do not reject the null.\n",
    "\n",
    "### Selection Mechanisms \n",
    "\n",
    "We need to throw out columns while building a model, \n",
    "- if it has a lot of column then the model will be unreliable, \n",
    "- even if we use many variables, we will have to explain the model to every person, \n",
    "\n",
    "and so we need to select the right variables.\n",
    "\n",
    "Methods are:\n",
    "\n",
    "1. **All in method** : we use it when we have a prior knowledge that we need to use these set of variables to obtain optimum results, or we have to use all the variables.\n",
    "\n",
    "\n",
    "2. **Backward Elimination** : \n",
    "- Step 1: Select a significance level to stay in the model (Eg: SL=0.05).\n",
    "- Step 2: Fit the full model with the possible predictors.\n",
    "- Step 3: Consider the predictor with the highest p-value. If P>SL, go to step 4 otherwise your model is completed.\n",
    "- Step 4: Remove the predictor.\n",
    "- Step 5: Fit models without this variable and move to step 3.\n",
    "\n",
    "3. **Forward Selection** :\n",
    "- Step 1: Select a significance level to enter in the model (Eg: SL=0.05)\n",
    "- Step 2: Fit all Simple Regression models y ~ Xn, select the one with the lowest p-value.\n",
    "- Step 3: Keep the variable and fit all possible models with one extra predictor added to the one(s) you already have.\n",
    "- Step 4: Consider the predictor with the lowest p-value. If **p-value < SL, go to Step3**, else the model is completed.\n",
    "- We will only stop when P>SL, the variable is not significant anymore and select the previous model.\n",
    "\n",
    "4. **Bidirectional Elimination** :\n",
    "- Step 1: Select a significance level to stay in the model (Eg: SLENTRY=0.05, SLSTAY=0.05)\n",
    "- Step 2: Perform the next step of forward selection (new variables must have p-value<SLENTT to enter)\n",
    "- Step 3: Perform all steps of backward elimination (old variables must have p-value<SLSTAY to stay) repeat step 2 and 3 until no new variables than move to step 5.\n",
    "- Step 4: If no new variables can enter and no variables can exit, so it is the final model.\n",
    "\n",
    "5. **All Possible Models**\n",
    "- Step 1: Select the criterion of the goodness of fit.\n",
    "- Step 2: Construct all possible regression models 2N−1 total combinations.\n",
    "- Step 3: Select one with the best criterion.\n",
    "\n",
    "But this is bad because it will have to go through each and every column in the data set which build exponential number of model which is resouce consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cc5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to understand how each variable is related to the profit.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb3b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "x = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c7fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])],remainder='passthrough')\n",
    "#we specify what kind of transformation we want to do, then what kind of encoding and third the indexes of the column\n",
    "#passthrough means keeping columns that wont be transformed\n",
    "#we will use fit transform which will fit the connection and transform the column \n",
    "x = np.array(ct.fit_transform(x))\n",
    "#fit transform doesnt return the data as numpy array and so we convert it using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265a41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d104c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98310c",
   "metadata": {},
   "source": [
    "We dont need to bother about dummy variable because those are ignored by out class.\n",
    "\n",
    "The model will also automatically identify the features that have the highest P values or that are the most statistically significant to figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307e988f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9495a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "#We will first show the actual test set and then the predicted result to show the accuracy\n",
    "y_pred = mlr.predict(x_test)\n",
    "np.set_printoptions(precision=2)\n",
    "# concatenate two vectors of real profits and predicted profits\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))\n",
    "#So second value that is axis, here can take two values, \n",
    "#zero means that we want to do a vertical concatenation,and one means that we want to do a horizontal concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb33dd",
   "metadata": {},
   "source": [
    "Backward Elimination is irrelevant in Python, because the Scikit-Learn library automatically takes care of selecting the statistically significant features when training the model to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf233ce",
   "metadata": {},
   "source": [
    "**Making a single prediction (for example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = 'California')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d3e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182266.29]\n"
     ]
    }
   ],
   "source": [
    "print(mlr.predict([[0.0, 0.0, 1.0, 160000, 130000, 300000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdd25e",
   "metadata": {},
   "source": [
    "**How do I get the final regression equation y = b0 + b1 x1 + b2 x2 + ... with the final values of the coefficients?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1951379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.66e+01 -8.73e+02  7.86e+02  7.73e-01  3.29e-02  3.66e-02]\n",
      "42467.52924854249\n"
     ]
    }
   ],
   "source": [
    "print(mlr.coef_)\n",
    "print(mlr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233d71f",
   "metadata": {},
   "source": [
    "$$\\textrm{Profit} = 86.6 \\times \\textrm{Dummy State 1} - 873 \\times \\textrm{Dummy State 2} + 786 \\times \\textrm{Dummy State 3} + 0.773 \\times \\textrm{R&D Spend} + 0.0329 \\times \\textrm{Administration} + 0.0366 \\times \\textrm{Marketing Spend} + 42467.53$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
